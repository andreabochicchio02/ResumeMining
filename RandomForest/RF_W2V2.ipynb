{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d385f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\filip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from gensim.models import Word2Vec\n",
    "from imblearn.pipeline import Pipeline\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Download necessary NLTK packages\n",
    "try:\n",
    "    nltk.download('punkt_tab')\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante il download dei pacchetti NLTK: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../PreProcessingResumes/processed_data/Resume_proc_lemm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94677410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['Resume_str'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e23454",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "594dfa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di parole nel vocabolario Word2Vec: 19857\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numero di parole nel vocabolario Word2Vec: {len(w2v_model.wv.index_to_key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74eafc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per ottenere i vettori concatenati da tutte le parole del documento\n",
    "def get_word2vec_vector(tokens, model, vector_size=10, max_length=100):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    \n",
    "    # Se non ci sono parole nel vocabolario\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(vector_size * max_length)  # Padding con vettori di zeri\n",
    "\n",
    "    # Concatenazione dei vettori delle parole\n",
    "    concatenated_vectors = np.concatenate(vectors)\n",
    "\n",
    "    # Se il numero di vettori concatenati è inferiore a max_length, aggiungiamo padding\n",
    "    if len(concatenated_vectors) < vector_size * max_length:\n",
    "        padding = np.zeros(vector_size * (max_length - len(vectors)))\n",
    "        concatenated_vectors = np.concatenate([concatenated_vectors, padding])\n",
    "    \n",
    "    # Se il numero di vettori concatenati è maggiore di max_length, tagliamo la concatenazione\n",
    "    if len(concatenated_vectors) > vector_size * max_length:\n",
    "        concatenated_vectors = concatenated_vectors[:vector_size * max_length]\n",
    "    \n",
    "    return concatenated_vectors\n",
    "\n",
    "df['vector'] = df['tokens'].apply(lambda x: get_word2vec_vector(x, w2v_model, vector_size=100, max_length=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8f975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(df['vector'].values)\n",
    "y = df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9391c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35b5574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: (1986, 50000)\n",
      "Validation data size: (497, 50000)\n"
     ]
    }
   ],
   "source": [
    "# Print the sizes of the split datasets\n",
    "print(\"Train data size:\", X_train.shape)\n",
    "print(\"Validation data size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369b6f3",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0b0cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results[\"rank_test_score\"] == i)\n",
    "        for candidate in candidates:\n",
    "            print(f\"Model with rank: {i}\")\n",
    "            print(f\"Mean validation score: {results['mean_test_score'][candidate]:.3f} (std: {results['std_test_score'][candidate]:.3f})\")\n",
    "            print(f\"Parameters: {results['params'][candidate]}\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "364647fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a42d3722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applicazione di SMOTE alle classi: ['ARTS', 'TEACHER', 'APPAREL', 'DIGITAL-MEDIA', 'AGRICULTURE', 'AUTOMOBILE', 'BPO']\n"
     ]
    }
   ],
   "source": [
    "class_counts = y_train.value_counts()\n",
    "min_classes = class_counts[class_counts < class_counts.mean()].index.tolist()\n",
    "\n",
    "print(f\"Applicazione di SMOTE alle classi: {min_classes}\")\n",
    "\n",
    "target_size = int(class_counts.max())\n",
    "sampling_strategy = {cls: target_size for cls in min_classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62111741",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('clf', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e7d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Definisci i parametri per il grid search\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [400, 700],\n",
    "    'clf__max_features': ['sqrt', 'log2'],\n",
    "    'clf__max_depth' : [8, 9, 10],\n",
    "    'clf__criterion' : ['gini', 'entropy'],\n",
    "    'clf__min_samples_split': [10],\n",
    "    'clf__min_samples_leaf': [15]\n",
    "}\n",
    "\n",
    "# 3. StratifiedKFold mantiene lo sbilanciamento nei fold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 4. GridSearchCV con cross-validation interna e pipeline\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=skf,\n",
    "    scoring='f1_weighted',\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time()\n",
    "grid_search = grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"GridSearchCV took {(time() - start):.2f} seconds for {len(grid_search.cv_results_['params'])} candidates parameter settings.\\n\")\n",
    "report(grid_search.cv_results_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d678ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame con i risultati\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "param_cols = [col for col in results.columns if col.startswith(\"param_\")]\n",
    "\n",
    "# Converti tutti i parametri in stringa per la pivot table\n",
    "for p in param_cols:\n",
    "    results[p] = results[p].astype(str)\n",
    "\n",
    "# Heatmap per ogni coppia di parametri\n",
    "for p1, p2 in combinations(param_cols, 2):\n",
    "    try:\n",
    "        pivot_val = results.pivot_table(values='mean_test_score', index=p1, columns=p2)\n",
    "        pivot_train = results.pivot_table(values='mean_train_score', index=p1, columns=p2)\n",
    "\n",
    "        if pivot_val.shape[0] > 1 and pivot_val.shape[1] > 1:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "            sns.heatmap(pivot_train, annot=True, fmt=\".3f\", cmap=\"Blues\", ax=axes[0])\n",
    "            axes[0].set_title(f\"Train Score\")\n",
    "            axes[0].set_xlabel(p2.replace(\"param_\", \"\"))\n",
    "            axes[0].set_ylabel(p1.replace(\"param_\", \"\"))\n",
    "\n",
    "            sns.heatmap(pivot_val, annot=True, fmt=\".3f\", cmap=\"Greens\", ax=axes[1])\n",
    "            axes[1].set_title(f\"Validation Score\")\n",
    "            axes[1].set_xlabel(p2.replace(\"param_\", \"\"))\n",
    "            axes[1].set_ylabel(\"\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Errore con {p1} e {p2}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65eecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_Model_best = grid_search.best_estimator_\n",
    "prediction = RF_Model_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b46356",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Score: {:.2f}\".format(RF_Model_best.score(X_train, y_train)))\n",
    "print(\"Test Score: {:.2f}\".format(RF_Model_best.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f7276",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))  # Imposta la dimensione della figura\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, prediction)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
