{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../PreProcessing/processed_data/Resume_proc_lemm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    binary=False,\n",
    "    max_features=10000,             # come prima, valuta se serve\n",
    "    ngram_range=(1, 6),             # unigrams fino a 6-grams\n",
    "    max_df=0.8,\n",
    "    min_df=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: (1986,)\n",
      "Validation data size: (497,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Resume_str'], df['Category'], test_size=0.2, random_state=42, stratify=df['Category'])\n",
    "\n",
    "print(\"Train data size:\", X_train.shape)\n",
    "print(\"Validation data size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_features = tfidf_vect.fit_transform(X_train)\n",
    "#X_test_features = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results[\"rank_test_score\"] == i)\n",
    "        for candidate in candidates:\n",
    "            print(f\"Model with rank: {i}\")\n",
    "            print(f\"Mean validation score: {results['mean_test_score'][candidate]:.3f} (std: {results['std_test_score'][candidate]:.3f})\")\n",
    "            print(f\"Parameters: {results['params'][candidate]}\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applicazione di SMOTE alle classi: ['ARTS', 'TEACHER', 'APPAREL', 'DIGITAL-MEDIA', 'AGRICULTURE', 'AUTOMOBILE', 'BPO']\n"
     ]
    }
   ],
   "source": [
    "# SMOTE\n",
    "class_counts = y_train.value_counts()\n",
    "min_classes = class_counts[class_counts < class_counts.mean()].index.tolist()\n",
    "\n",
    "print(f\"Applicazione di SMOTE alle classi: {min_classes}\")\n",
    "\n",
    "target_size = int(class_counts.max())\n",
    "sampling_strategy = {cls: target_size for cls in min_classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline: TF-IDF + Scaling + Smote + SVM\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', tfidf_vect),\n",
    "    ('smote', SMOTE(sampling_strategy=sampling_strategy, random_state=42)),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'clf__C': [0.01, 0.1, 1, 10],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__solver': ['lbfgs']\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=skf,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "start = time()\n",
    "grid_search = grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"GridSearchCV took {(time() - start):.2f} seconds for {len(grid_search.cv_results_['params'])} candidates parameter settings.\\n\")\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Calcola la differenza tra train e validation scores\n",
    "results['overfit_gap'] = results['mean_train_score'] - results['mean_test_score']\n",
    "\n",
    "# Ora puoi:\n",
    "# - Ordinare per \"overfit_gap\" crescente\n",
    "# - Oppure selezionare i modelli che hanno \"overfit_gap\" piÃ¹ basso\n",
    "results_sorted = results.sort_values(by='overfit_gap')\n",
    "\n",
    "# Mostra i primi\n",
    "results_sorted[['params', 'mean_train_score', 'mean_test_score', 'overfit_gap']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame con i risultati\n",
    "param_cols = [col for col in results.columns if col.startswith(\"param_\")]\n",
    "\n",
    "# Converti tutti i parametri in stringa per la pivot table\n",
    "for p in param_cols:\n",
    "    results[p] = results[p].astype(str)\n",
    "\n",
    "# Heatmap per ogni coppia di parametri\n",
    "for p1, p2 in combinations(param_cols, 2):\n",
    "    try:\n",
    "        pivot_val = results.pivot_table(values='mean_test_score', index=p1, columns=p2)\n",
    "        pivot_train = results.pivot_table(values='mean_train_score', index=p1, columns=p2)\n",
    "\n",
    "        if pivot_val.shape[0] > 1 and pivot_val.shape[1] > 1:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "            sns.heatmap(pivot_train, annot=True, fmt=\".3f\", cmap=\"Blues\", ax=axes[0])\n",
    "            axes[0].set_title(f\"Train Score\")\n",
    "            axes[0].set_xlabel(p2.replace(\"param_\", \"\"))\n",
    "            axes[0].set_ylabel(p1.replace(\"param_\", \"\"))\n",
    "\n",
    "            sns.heatmap(pivot_val, annot=True, fmt=\".3f\", cmap=\"Greens\", ax=axes[1])\n",
    "            axes[1].set_title(f\"Validation Score\")\n",
    "            axes[1].set_xlabel(p2.replace(\"param_\", \"\"))\n",
    "            axes[1].set_ylabel(\"\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Errore con {p1} e {p2}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_Model_best = grid_search.best_estimator_\n",
    "prediction = SVC_Model_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Score: {:.2f}\".format(SVC_Model_best.score(X_train, y_train)))\n",
    "print(\"Test Score: {:.2f}\".format(SVC_Model_best.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(y_test, prediction, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))  # Imposta la dimensione della figura\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, prediction)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_bin = label_binarize(y_test, classes=SVC_Model_best.classes_)\n",
    "y_score = SVC_Model_best.predict_proba(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i in range(len(SVC_Model_best.classes_)):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{SVC_Model_best.classes_[i]} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves (One-vs-Rest)\")\n",
    "\n",
    "# Sposta la legenda fuori dal grafico\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i in range(len(SVC_Model_best.classes_)):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    avg_precision = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "    plt.plot(recall, precision, label=f\"{SVC_Model_best.classes_[i]} (AP = {avg_precision:.2f})\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curves\")\n",
    "\n",
    "# Legenda esterna a destra\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
