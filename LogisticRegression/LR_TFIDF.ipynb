{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16852973</td>\n",
       "      <td>hr administrator marketing associate hr admini...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22323967</td>\n",
       "      <td>hr specialist u hr operation summary versatile...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33176873</td>\n",
       "      <td>hr director summary year experience recruiting...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27018550</td>\n",
       "      <td>hr specialist summary dedicated driven dynamic...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17812897</td>\n",
       "      <td>hr manager skill highlight hr skill hr departm...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         Resume_str Category\n",
       "0  16852973  hr administrator marketing associate hr admini...       HR\n",
       "1  22323967  hr specialist u hr operation summary versatile...       HR\n",
       "2  33176873  hr director summary year experience recruiting...       HR\n",
       "3  27018550  hr specialist summary dedicated driven dynamic...       HR\n",
       "4  17812897  hr manager skill highlight hr skill hr departm...       HR"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../PreProcessingResumes/processed_data/Resume_removeStopword_useLemm/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorizer that converts a collection of raw text documents into a matrix of TF-IDF features\n",
    "tfidf_vect = TfidfVectorizer(\n",
    "    lowercase=True,         # Convert all text to lowercase\n",
    "    binary=False,           # Use term frequency, not just presence/absence\n",
    "    max_features=10000,     # Keep only the top 10,000 most frequent terms\n",
    "    ngram_range=(1, 3),     # Extract n-grams\n",
    "    max_df=0.8,             # Ignore terms that appear in more than 80% of documents\n",
    "    min_df=2                # Ignore terms that appear in fewer than 2 documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df['Category'].value_counts()\n",
    "min_classes = class_counts[class_counts < 0.75*class_counts.mean()].index.tolist()\n",
    "\n",
    "print(f\"Applying SMOTE to classes: {min_classes}\")\n",
    "\n",
    "target_size = int(0.75*class_counts.max())\n",
    "sampling_strategy = {cls: target_size for cls in min_classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', tfidf_vect),\n",
    "    ('smote', SMOTE(sampling_strategy=sampling_strategy, random_state=42, k_neighbors=4)),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 279.99 seconds for 12 candidates parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation f1_weighted: 0.746 (std: 0.025)\n",
      "Mean validation accuracy: 0.741\n",
      "Parameters: {'clf__C': 1, 'clf__penalty': 'l1', 'clf__solver': 'saga'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation f1_weighted: 0.723 (std: 0.026)\n",
      "Mean validation accuracy: 0.720\n",
      "Parameters: {'clf__C': 1, 'clf__l1_ratio': 0.8, 'clf__penalty': 'elasticnet', 'clf__solver': 'saga'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation f1_weighted: 0.720 (std: 0.022)\n",
      "Mean validation accuracy: 0.719\n",
      "Parameters: {'clf__C': 1, 'clf__l1_ratio': 0.5, 'clf__penalty': 'elasticnet', 'clf__solver': 'saga'}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation f1_weighted: 0.709 (std: 0.020)\n",
      "Mean validation accuracy: 0.710\n",
      "Parameters: {'clf__C': 1, 'clf__l1_ratio': 0.2, 'clf__penalty': 'elasticnet', 'clf__solver': 'saga'}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation f1_weighted: 0.698 (std: 0.016)\n",
      "Mean validation accuracy: 0.701\n",
      "Parameters: {'clf__C': 1, 'clf__penalty': 'l2', 'clf__solver': 'saga'}\n",
      "\n",
      "Model with rank: 6\n",
      "Mean validation f1_weighted: 0.696 (std: 0.016)\n",
      "Mean validation accuracy: 0.700\n",
      "Parameters: {'clf__C': 1, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "Model with rank: 7\n",
      "Mean validation f1_weighted: 0.654 (std: 0.015)\n",
      "Mean validation accuracy: 0.671\n",
      "Parameters: {'clf__C': 0.1, 'clf__penalty': 'l2', 'clf__solver': 'saga'}\n",
      "\n",
      "Model with rank: 8\n",
      "Mean validation f1_weighted: 0.654 (std: 0.015)\n",
      "Mean validation accuracy: 0.670\n",
      "Parameters: {'clf__C': 0.1, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "Model with rank: 9\n",
      "Mean validation f1_weighted: 0.636 (std: 0.019)\n",
      "Mean validation accuracy: 0.656\n",
      "Parameters: {'clf__C': 0.1, 'clf__l1_ratio': 0.2, 'clf__penalty': 'elasticnet', 'clf__solver': 'saga'}\n",
      "\n",
      "Model with rank: 10\n",
      "Mean validation f1_weighted: 0.592 (std: 0.022)\n",
      "Mean validation accuracy: 0.620\n",
      "Parameters: {'clf__C': 0.1, 'clf__l1_ratio': 0.5, 'clf__penalty': 'elasticnet', 'clf__solver': 'saga'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'clf__C': [1e-1, 1],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__solver': ['lbfgs', 'saga']\n",
    "    },\n",
    "    {\n",
    "        'clf__C': [1e-1, 1],\n",
    "        'clf__penalty': ['l1'],\n",
    "        'clf__solver': ['saga']\n",
    "    },\n",
    "    {\n",
    "        'clf__C': [1e-1, 1],\n",
    "        'clf__penalty': ['elasticnet'],\n",
    "        'clf__solver': ['saga'],\n",
    "        'clf__l1_ratio': [0.2, 0.5, 0.8]\n",
    "    }\n",
    "]\n",
    "\n",
    "scoring = ['accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted']\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=skf,\n",
    "    scoring= scoring,\n",
    "    refit = False,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time()\n",
    "grid_search = grid.fit(df['Resume_str'], df['Category'])\n",
    "\n",
    "print(f\"GridSearchCV took {(time() - start):.2f} seconds for {len(grid_search.cv_results_['params'])} candidates parameter settings.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the cross-validation results from GridSearchCV into a pandas DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Sort the results by the weighted F1 score in descending order\n",
    "results_df = results_df.sort_values(by='mean_test_f1_weighted', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results_df)):\n",
    "    print(f\"\\n[{i+1}] Params: {results_df['params'][i]}\")\n",
    "    \n",
    "    for metric in scoring:\n",
    "        \n",
    "        print(f\"{metric.upper()}:\")\n",
    "        print(f\"\\tTRAIN:\", end='')\n",
    "        for fold in range(skf.get_n_splits()):\n",
    "            print(f\"\\tFold {fold+1}: {results_df[f'split{fold}_train_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\t\\tMean: {results_df[f'mean_train_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\tStd: {results_df[f'std_train_{metric}'][i]:.4f}\")\n",
    "\n",
    "        print(f\"\\tTEST:\", end='')\n",
    "        for fold in range(skf.get_n_splits()):\n",
    "            print(f\"\\tFold {fold+1}: {results_df[f'split{fold}_test_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\t\\tMean: {results_df[f'mean_test_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\tStd: {results_df[f'std_test_{metric}'][i]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in scoring:\n",
    "    data_test = {}\n",
    "    data_train = {}\n",
    "    for i in range(len(results_df)):\n",
    "        col_name = f'{i+1}'\n",
    "        data_test[col_name] = [results_df[f'split{fold}_test_{metric}'][i] for fold in range(skf.get_n_splits())]\n",
    "        data_train[col_name] = [results_df[f'split{fold}_train_{metric}'][i] for fold in range(skf.get_n_splits())]\n",
    "\n",
    "    df_test = pd.DataFrame(data_test)\n",
    "    df_train = pd.DataFrame(data_train)\n",
    "\n",
    "    means_test = df_test.mean()\n",
    "    stds_test = df_test.std()\n",
    "    means_train = df_train.mean()\n",
    "    stds_train = df_train.std()\n",
    "\n",
    "    plt.figure(figsize=(max(6, len(results_df) * 0.75), 4))\n",
    "\n",
    "    plt.errorbar(means_test.index, means_test.values, yerr=stds_test.values, fmt='-o', capsize=5, label='Test', color='blue')\n",
    "\n",
    "    plt.errorbar(means_train.index, means_train.values, yerr=stds_train.values, fmt='--s', capsize=5, label='Train', color='orange')\n",
    "\n",
    "    plt.ylabel(f'{metric} score')\n",
    "    plt.xlabel('Combination of parameters')\n",
    "    plt.title(f'Model comparison on {metric} (mean ± std)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in scoring:\n",
    "    data = {}\n",
    "    for i in range(len(results_df)):\n",
    "        col_name = f'{i+1}'\n",
    "        data[col_name] = [results_df[f'split{fold}_test_{metric}'][i] for fold in range(skf.get_n_splits())]\n",
    "\n",
    "    df_metric = pd.DataFrame(data)\n",
    "    ax = df_metric.boxplot(figsize=(max(6, len(results_df) * 0.75), 4))\n",
    "    ax.set_ylabel(f'{metric} score')\n",
    "    ax.set_xlabel('Combination of parameters')\n",
    "    plt.title(f'Model comparison on {metric}')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
