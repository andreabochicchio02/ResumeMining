{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Download necessary NLTK packages\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0ad0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16852973</td>\n",
       "      <td>hr administrator marketing associate hr admini...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22323967</td>\n",
       "      <td>hr specialist us hr operations summary versati...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33176873</td>\n",
       "      <td>hr director summary over years experience in r...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27018550</td>\n",
       "      <td>hr specialist summary dedicated driven and dyn...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17812897</td>\n",
       "      <td>hr manager skill highlights hr skills hr depar...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         Resume_str Category\n",
       "0  16852973  hr administrator marketing associate hr admini...       HR\n",
       "1  22323967  hr specialist us hr operations summary versati...       HR\n",
       "2  33176873  hr director summary over years experience in r...       HR\n",
       "3  27018550  hr specialist summary dedicated driven and dyn...       HR\n",
       "4  17812897  hr manager skill highlights hr skills hr depar...       HR"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../PreProcessingResumes/processed_data/Resume_removeStopword_useLemm/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f06871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=200, window=10, min_count=2, dm=1, epochs=20):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.dm = dm\n",
    "        self.epochs = epochs\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        tagged_data = [\n",
    "            TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) \n",
    "            for i, doc in enumerate(X)\n",
    "        ]\n",
    "        self.model = Doc2Vec(\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            dm=self.dm,\n",
    "            epochs=self.epochs\n",
    "        )\n",
    "        self.model.build_vocab(tagged_data)\n",
    "        self.model.train(tagged_data, total_examples=self.model.corpus_count, epochs=self.model.epochs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.model.infer_vector(word_tokenize(doc.lower()), epochs=20) for doc in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0696c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2VecTransformer(\n",
    "    vector_size=200,  # Dimensionality of the document vectors\n",
    "    window=10,        # Number of context words to consider to the left and right of the target word\n",
    "    min_count=2,      # Ignores all words with total frequency lower than this\n",
    "    dm=1,             # Defines the training algorithm: 1 means Distributed Memory, 0 means Distributed Bag of Words\n",
    "    epochs=20         # Number of training epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfff6b9",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "364647fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabbfecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('d2v', d2v_model),\n",
    "    ('smote', SMOTE(sampling_strategy=0.8, random_state=42, k_neighbors=4)),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7424bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 5.83 seconds for 4 candidates parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation f1_weighted: 0.577 (std: 0.011)\n",
      "Mean validation accuracy: 0.579\n",
      "Parameters: {'clf__C': 0.01, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation f1_weighted: 0.551 (std: 0.015)\n",
      "Mean validation accuracy: 0.552\n",
      "Parameters: {'clf__C': 0.1, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation f1_weighted: 0.525 (std: 0.005)\n",
      "Mean validation accuracy: 0.525\n",
      "Parameters: {'clf__C': 1, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation f1_weighted: 0.508 (std: 0.006)\n",
      "Mean validation accuracy: 0.509\n",
      "Parameters: {'clf__C': 10, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "{'clf__C': 0.01, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'clf__C': [0.01, 0.1, 1, 10],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__solver': ['lbfgs']\n",
    "    }\n",
    "]\n",
    "\n",
    "scoring = ['accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted']\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=skf,\n",
    "    scoring= scoring,\n",
    "    refit = False,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time()\n",
    "grid_search = grid.fit(df['Resume_str'], df['Category'])\n",
    "\n",
    "print(f\"GridSearchCV took {(time() - start):.2f} seconds for {len(grid_search.cv_results_['params'])} candidates parameter settings.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the cross-validation results from GridSearchCV into a pandas DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Sort the results by the weighted F1 score in descending order\n",
    "results_df = results_df.sort_values(by='mean_test_f1_weighted', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d99c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results_df)):\n",
    "    print(f\"\\n[{i+1}] Params: {results_df['params'][i]}\")\n",
    "    \n",
    "    for metric in scoring:\n",
    "        \n",
    "        print(f\"{metric.upper()}:\")\n",
    "        print(f\"\\tTRAIN:\", end='')\n",
    "        for fold in range(skf.get_n_splits()):\n",
    "            print(f\"\\tFold {fold+1}: {results_df[f'split{fold}_train_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\t\\tMean: {results_df[f'mean_train_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\tStd: {results_df[f'std_train_{metric}'][i]:.4f}\")\n",
    "\n",
    "        print(f\"\\tTEST:\", end='')\n",
    "        for fold in range(skf.get_n_splits()):\n",
    "            print(f\"\\tFold {fold+1}: {results_df[f'split{fold}_test_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\t\\tMean: {results_df[f'mean_test_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\tStd: {results_df[f'std_test_{metric}'][i]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in scoring:\n",
    "    data_test = {}\n",
    "    data_train = {}\n",
    "    for i in range(len(results_df)):\n",
    "        col_name = f'{i+1}'\n",
    "        data_test[col_name] = [results_df[f'split{fold}_test_{metric}'][i] for fold in range(skf.get_n_splits())]\n",
    "        data_train[col_name] = [results_df[f'split{fold}_train_{metric}'][i] for fold in range(skf.get_n_splits())]\n",
    "\n",
    "    df_test = pd.DataFrame(data_test)\n",
    "    df_train = pd.DataFrame(data_train)\n",
    "\n",
    "    means_test = df_test.mean()\n",
    "    stds_test = df_test.std()\n",
    "    means_train = df_train.mean()\n",
    "    stds_train = df_train.std()\n",
    "\n",
    "    plt.figure(figsize=(max(6, len(results_df) * 0.75), 4))\n",
    "\n",
    "    plt.errorbar(means_test.index, means_test.values, yerr=stds_test.values, fmt='-o', capsize=5, label='Test', color='blue')\n",
    "\n",
    "    plt.errorbar(means_train.index, means_train.values, yerr=stds_train.values, fmt='--s', capsize=5, label='Train', color='orange')\n",
    "\n",
    "    plt.ylabel(f'{metric} score')\n",
    "    plt.xlabel('Combination of parameters')\n",
    "    plt.title(f'Model comparison on {metric} (mean ± std)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in scoring:\n",
    "    data = {}\n",
    "    for i in range(len(results_df)):\n",
    "        col_name = f'{i+1}'\n",
    "        data[col_name] = [results_df[f'split{fold}_test_{metric}'][i] for fold in range(skf.get_n_splits())]\n",
    "\n",
    "    df_metric = pd.DataFrame(data)\n",
    "    ax = df_metric.boxplot(figsize=(max(6, len(results_df) * 0.75), 4))\n",
    "    ax.set_ylabel(f'{metric} score')\n",
    "    ax.set_xlabel('Combination of parameters')\n",
    "    plt.title(f'Model comparison on {metric}')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
