{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066cbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe9bede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16852973</td>\n",
       "      <td>hr administrator marketing associate hr admini...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22323967</td>\n",
       "      <td>hr specialist us hr operations summary versati...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33176873</td>\n",
       "      <td>hr director summary over years experience in r...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27018550</td>\n",
       "      <td>hr specialist summary dedicated driven and dyn...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17812897</td>\n",
       "      <td>hr manager skill highlights hr skills hr depar...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         Resume_str Category\n",
       "0  16852973  hr administrator marketing associate hr admini...       HR\n",
       "1  22323967  hr specialist us hr operations summary versati...       HR\n",
       "2  33176873  hr director summary over years experience in r...       HR\n",
       "3  27018550  hr specialist summary dedicated driven and dyn...       HR\n",
       "4  17812897  hr manager skill highlights hr skills hr depar...       HR"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../PreProcessingResumes/processed_data/Resume/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820dcadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert resumes column to a list\n",
    "resumes = df[\"Resume_str\"].tolist()\n",
    "    \n",
    "# Encode category labels as integers\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(df[\"Category\"])\n",
    "category_names = encoder.classes_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d21bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb45fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d832da682fb443bb36e20d1685b7731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae9cbee5c374e3783dd24a8d1619a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate SBERT embeddings\n",
    "resumes_embed = sbert_model.encode(resumes, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09479578",
   "metadata": {},
   "source": [
    "### Logistic Regression Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e605469",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5cd5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('smote', SMOTE(sampling_strategy=0.8, random_state=42, k_neighbors=4)),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad18cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 14.29 seconds for 4 candidates parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation f1_weighted: 0.760 (std: 0.012)\n",
      "Mean validation accuracy: 0.759\n",
      "Parameters: {'clf__C': 0.01, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation f1_weighted: 0.743 (std: 0.007)\n",
      "Mean validation accuracy: 0.745\n",
      "Parameters: {'clf__C': 0.1, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation f1_weighted: 0.730 (std: 0.007)\n",
      "Mean validation accuracy: 0.732\n",
      "Parameters: {'clf__C': 1, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation f1_weighted: 0.718 (std: 0.005)\n",
      "Mean validation accuracy: 0.722\n",
      "Parameters: {'clf__C': 10, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'clf__C': [0.01, 0.1, 1, 10],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__solver': ['lbfgs']\n",
    "    }\n",
    "]\n",
    "\n",
    "scoring = ['accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted']\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=skf,\n",
    "    scoring= scoring,\n",
    "    refit = False,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time()\n",
    "grid_search = grid.fit(resumes_embed, labels)\n",
    "\n",
    "print(f\"GridSearchCV took {(time() - start):.2f} seconds for {len(grid_search.cv_results_['params'])} candidates parameter settings.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the cross-validation results from GridSearchCV into a pandas DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Sort the results by the weighted F1 score in descending order\n",
    "results_df = results_df.sort_values(by='mean_test_f1_weighted', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cdaa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results_df)):\n",
    "    print(f\"\\n[{i+1}] Params: {results_df['params'][i]}\")\n",
    "    \n",
    "    for metric in scoring:\n",
    "        \n",
    "        print(f\"{metric.upper()}:\")\n",
    "        print(f\"\\tTRAIN:\", end='')\n",
    "        for fold in range(skf.get_n_splits()):\n",
    "            print(f\"\\tFold {fold+1}: {results_df[f'split{fold}_train_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\t\\tMean: {results_df[f'mean_train_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\tStd: {results_df[f'std_train_{metric}'][i]:.4f}\")\n",
    "\n",
    "        print(f\"\\tTEST:\", end='')\n",
    "        for fold in range(skf.get_n_splits()):\n",
    "            print(f\"\\tFold {fold+1}: {results_df[f'split{fold}_test_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\t\\tMean: {results_df[f'mean_test_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\tStd: {results_df[f'std_test_{metric}'][i]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d736a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in scoring:\n",
    "    data_test = {}\n",
    "    data_train = {}\n",
    "    for i in range(len(results_df)):\n",
    "        col_name = f'{i+1}'\n",
    "        data_test[col_name] = [results_df[f'split{fold}_test_{metric}'][i] for fold in range(skf.get_n_splits())]\n",
    "        data_train[col_name] = [results_df[f'split{fold}_train_{metric}'][i] for fold in range(skf.get_n_splits())]\n",
    "\n",
    "    df_test = pd.DataFrame(data_test)\n",
    "    df_train = pd.DataFrame(data_train)\n",
    "\n",
    "    means_test = df_test.mean()\n",
    "    stds_test = df_test.std()\n",
    "    means_train = df_train.mean()\n",
    "    stds_train = df_train.std()\n",
    "\n",
    "    plt.figure(figsize=(max(6, len(results_df) * 0.75), 4))\n",
    "\n",
    "    plt.errorbar(means_test.index, means_test.values, yerr=stds_test.values, fmt='-o', capsize=5, label='Test', color='blue')\n",
    "\n",
    "    plt.errorbar(means_train.index, means_train.values, yerr=stds_train.values, fmt='--s', capsize=5, label='Train', color='orange')\n",
    "\n",
    "    plt.ylabel(f'{metric} score')\n",
    "    plt.xlabel('Combination of parameters')\n",
    "    plt.title(f'Model comparison on {metric} (mean Â± std)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in scoring:\n",
    "    data = {}\n",
    "    for i in range(len(results_df)):\n",
    "        col_name = f'{i+1}'\n",
    "        data[col_name] = [results_df[f'split{fold}_test_{metric}'][i] for fold in range(skf.get_n_splits())]\n",
    "\n",
    "    df_metric = pd.DataFrame(data)\n",
    "    ax = df_metric.boxplot(figsize=(max(6, len(results_df) * 0.75), 4))\n",
    "    ax.set_ylabel(f'{metric} score')\n",
    "    ax.set_xlabel('Combination of parameters')\n",
    "    plt.title(f'Model comparison on {metric}')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
