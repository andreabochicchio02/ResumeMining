{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\filip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from time import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from gensim.models import Word2Vec\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Download necessary NLTK packages\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../PreProcessingResumes/processed_data/Resume_removeStopword_useLemm/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedW2VTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=200, window=10, min_count=2, epochs=40):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.epochs = epochs\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.w2v_model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.tokens_list_ = [word_tokenize(doc) for doc in X]\n",
    "        self.tfidf_matrix_ = self.tfidf.fit_transform(X)            # Fit TF-IDF\n",
    "        self.w2v_model = Word2Vec(\n",
    "            sentences=self.tokens_list_,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            epochs=self.epochs\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        vectors = []\n",
    "        tfidf_matrix = self.tfidf.transform(X)\n",
    "        for idx, doc in enumerate(X):\n",
    "            tokens = word_tokenize(doc)\n",
    "            vec = self._get_doc_vector(tokens, tfidf_matrix[idx, :])\n",
    "            vectors.append(vec)\n",
    "        return np.vstack(vectors)\n",
    "\n",
    "    def _get_doc_vector(self, tokens, tfidf_row):\n",
    "        word2idx = self.tfidf.vocabulary_\n",
    "        vecs, weights = [], []\n",
    "        for word in tokens:\n",
    "            if word in self.w2v_model.wv and word in word2idx:\n",
    "                vecs.append(self.w2v_model.wv[word])\n",
    "                weights.append(tfidf_row[0, word2idx[word]])\n",
    "        if not vecs:\n",
    "            return np.zeros(self.vector_size)\n",
    "        vecs = np.array(vecs)\n",
    "        w = np.array(weights)\n",
    "        w = w / w.sum() if w.sum() > 0 else np.ones_like(w) / len(w)    # Weighted stats\n",
    "        stats = [\n",
    "            np.average(vecs, axis=0, weights=w)\n",
    "        ]\n",
    "        return np.concatenate(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c94c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "advancedW2V = AdvancedW2VTransformer(\n",
    "    vector_size=50, \n",
    "    window=10, \n",
    "    min_count=2, \n",
    "    epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369b6f3",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "364647fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df['Category'].value_counts()\n",
    "min_classes = class_counts[class_counts < 0.75*class_counts.mean()].index.tolist()\n",
    "\n",
    "print(f\"Applying SMOTE to classes: {min_classes}\")\n",
    "\n",
    "target_size = int(0.75*class_counts.max())\n",
    "sampling_strategy = {cls: target_size for cls in min_classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ca21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('w2v', advancedW2V),\n",
    "    ('smote', SMOTE(sampling_strategy=sampling_strategy, random_state=42, k_neighbors=4)),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf107f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 7.76 seconds for 4 candidates parameter settings.\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.516 (std: 0.018)\n",
      "Parameters: {'clf__C': 0.01, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.493 (std: 0.019)\n",
      "Parameters: {'clf__C': 0.1, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.467 (std: 0.025)\n",
      "Parameters: {'clf__C': 1, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.456 (std: 0.016)\n",
      "Parameters: {'clf__C': 10, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'clf__C': [0.01, 0.1, 1, 10],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__solver': ['lbfgs']\n",
    "    }\n",
    "]\n",
    "\n",
    "scoring = ['accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted']\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=skf,\n",
    "    scoring= scoring,\n",
    "    refit = False,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time()\n",
    "grid_search = grid.fit(df['Resume_str'], df['Category'])\n",
    "\n",
    "print(f\"GridSearchCV took {(time() - start):.2f} seconds for {len(grid_search.cv_results_['params'])} candidates parameter settings.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b30ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the cross-validation results from GridSearchCV into a pandas DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Sort the results by the weighted F1 score in descending order\n",
    "results_df = results_df.sort_values(by='mean_test_f1_weighted', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1280f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results_df)):\n",
    "    print(f\"\\n[{i+1}] Params: {results_df['params'][i]}\")\n",
    "    \n",
    "    for metric in scoring:\n",
    "        \n",
    "        print(f\"{metric.upper()}:\")\n",
    "        print(f\"\\tTRAIN:\", end='')\n",
    "        for fold in range(skf.get_n_splits()):\n",
    "            print(f\"\\tFold {fold+1}: {results_df[f'split{fold}_train_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\t\\tMean: {results_df[f'mean_train_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\tStd: {results_df[f'std_train_{metric}'][i]:.4f}\")\n",
    "\n",
    "        print(f\"\\tTEST:\", end='')\n",
    "        for fold in range(skf.get_n_splits()):\n",
    "            print(f\"\\tFold {fold+1}: {results_df[f'split{fold}_test_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\t\\tMean: {results_df[f'mean_test_{metric}'][i]:.4f}\", end='')\n",
    "        print(f\"\\tStd: {results_df[f'std_test_{metric}'][i]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af49555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in scoring:\n",
    "    data_train = {}\n",
    "    data_test = {}\n",
    "    for i in range(len(results_df)):\n",
    "        col_name = f'{i+1}'\n",
    "        data_test[col_name] = [results_df[f'split{fold}_test_{metric}'][i] for fold in range(skf.get_n_splits())]\n",
    "        data_train[col_name] = [results_df[f'split{fold}_train_{metric}'][i] for fold in range(skf.get_n_splits())]\n",
    "\n",
    "    df_train = pd.DataFrame(data_train)\n",
    "    df_test = pd.DataFrame(data_test)\n",
    "\n",
    "    means_test = df_test.mean()\n",
    "    stds_test = df_test.std()\n",
    "    means_train = df_train.mean()\n",
    "    stds_train = df_train.std()\n",
    "\n",
    "    plt.figure(figsize=(max(6, len(results_df) * 0.75), 4))\n",
    "\n",
    "    plt.errorbar(means_test.index, means_test.values, yerr=stds_test.values, fmt='-o', capsize=5, label='Test', color='blue')\n",
    "\n",
    "    plt.errorbar(means_train.index, means_train.values, yerr=stds_train.values, fmt='--s', capsize=5, label='Train', color='orange')\n",
    "\n",
    "    plt.ylabel(f'{metric} score')\n",
    "    plt.xlabel('Combination of parameters')\n",
    "    plt.title(f'Model comparison on {metric} (mean ± std)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797eee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in scoring:\n",
    "    data = {}\n",
    "    for i in range(len(results_df)):\n",
    "        col_name = f'{i+1}'\n",
    "        data[col_name] = [results_df[f'split{fold}_test_{metric}'][i] for fold in range(skf.get_n_splits())]\n",
    "\n",
    "    df_metric = pd.DataFrame(data)\n",
    "    ax = df_metric.boxplot(figsize=(max(6, len(results_df) * 0.75), 4))\n",
    "    ax.set_ylabel(f'{metric} score')\n",
    "    ax.set_xlabel('Combination of parameters')\n",
    "    plt.title(f'Model comparison on {metric}')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
