{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download necessary NLTK packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>Resume_html</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16852973</td>\n",
       "      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22323967</td>\n",
       "      <td>HR SPECIALIST, US HR OPERATIONS      ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33176873</td>\n",
       "      <td>HR DIRECTOR       Summary      Over 2...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27018550</td>\n",
       "      <td>HR SPECIALIST       Summary    Dedica...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17812897</td>\n",
       "      <td>HR MANAGER         Skill Highlights  ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         Resume_str  \\\n",
       "0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
       "1  22323967           HR SPECIALIST, US HR OPERATIONS      ...   \n",
       "2  33176873           HR DIRECTOR       Summary      Over 2...   \n",
       "3  27018550           HR SPECIALIST       Summary    Dedica...   \n",
       "4  17812897           HR MANAGER         Skill Highlights  ...   \n",
       "\n",
       "                                         Resume_html Category  \n",
       "0  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "1  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "2  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "3  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "4  <div class=\"fontsize fontface vmargins hmargin...       HR  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/DataSetKaggle/Resume/Resume.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16852973</td>\n",
       "      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22323967</td>\n",
       "      <td>HR SPECIALIST, US HR OPERATIONS      ...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33176873</td>\n",
       "      <td>HR DIRECTOR       Summary      Over 2...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27018550</td>\n",
       "      <td>HR SPECIALIST       Summary    Dedica...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17812897</td>\n",
       "      <td>HR MANAGER         Skill Highlights  ...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         Resume_str Category\n",
       "0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...       HR\n",
       "1  22323967           HR SPECIALIST, US HR OPERATIONS      ...       HR\n",
       "2  33176873           HR DIRECTOR       Summary      Over 2...       HR\n",
       "3  27018550           HR SPECIALIST       Summary    Dedica...       HR\n",
       "4  17812897           HR MANAGER         Skill Highlights  ...       HR"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns = ['Resume_html'], inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2484 entries, 0 to 2483\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   ID          2484 non-null   int64 \n",
      " 1   Resume_str  2484 non-null   object\n",
      " 2   Category    2484 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 58.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resume_str    1\n",
       "Category      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the data visualization, we noticed that there is an empty resume in the dataset\n",
    "df.select_dtypes(include='object').apply(lambda col: col.str.strip() == '').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Resume_str'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2483 entries, 0 to 2483\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   ID          2483 non-null   int64 \n",
      " 1   Resume_str  2483 non-null   object\n",
      " 2   Category    2483 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 77.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of valid English words\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Get a list of stop word\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, use_lemmatization=True, use_stemming=False):\n",
    "    \"\"\"\n",
    "    Preprocesses the text: \n",
    "    - Converts to lowercase\n",
    "    - Removes punctuation\n",
    "    - Remove underscore\n",
    "    - Tokenizes\n",
    "    - Removes tokens with numbers\n",
    "    - Removes stopwords\n",
    "    - Applies lemmatization or stemming based on parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()                         # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)        # Remove all punctuation  \n",
    "    text = re.sub(r'_', ' ', text)              # Remove underscore\n",
    "    text = re.sub('\\s+', ' ', text)             # Remove multiple spaces\n",
    "    \n",
    "    tokens = word_tokenize(text) # Tokenize the text\n",
    "\n",
    "    processed_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Remove words containing numbers\n",
    "        if any(char.isdigit() for char in token):\n",
    "            continue\n",
    "\n",
    "        # Dictionary filtering\n",
    "        # if token not in english_words:\n",
    "        #    continue\n",
    "\n",
    "        # Remove stopwords\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "\n",
    "        if use_lemmatization:\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "        elif use_stemming:\n",
    "            token = stemmer.stem(token)\n",
    "\n",
    "        processed_tokens.append(token)\n",
    "    \n",
    "    return ' '.join(processed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_progress(df, column_name, **kwargs):\n",
    "\n",
    "    tqdm.pandas(desc=\"Processing Resumes\")  # Enable tqdm for Pandas\n",
    "    return df[column_name].progress_apply(lambda x: preprocess_text(x, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_sets = [\n",
    "    {\"use_lemmatization\": True, \"use_stemming\": True},\n",
    "    {\"use_lemmatization\": True, \"use_stemming\": False},\n",
    "    {\"use_lemmatization\": False, \"use_stemming\": True}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Resumes: 100%|██████████| 2483/2483 [00:25<00:00, 95.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame saved as: processed_data\\Resume_proc_lemm_stem.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Resumes: 100%|██████████| 2483/2483 [00:22<00:00, 108.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame saved as: processed_data\\Resume_proc_lemm.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Resumes: 100%|██████████| 2483/2483 [00:47<00:00, 51.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame saved as: processed_data\\Resume_proc_stem.csv\n"
     ]
    }
   ],
   "source": [
    "for params in parameter_sets:\n",
    "    # Create a copy of the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Apply preprocessing with the current parameter combination\n",
    "    df_copy['Resume_str'] = preprocess_text_with_progress(df_copy, 'Resume_str', **params)\n",
    "\n",
    "    # Define the folder where CSV files will be saved\n",
    "    output_folder = \"processed_data\"\n",
    "    \n",
    "    # Ensure the folder exists (create it if necessary)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Generate the full path for saving the file\n",
    "    if params[\"use_lemmatization\"] and not params[\"use_stemming\"]:\n",
    "        filename = os.path.join(output_folder, \"Resume_proc_lemm.csv\")\n",
    "    elif params[\"use_lemmatization\"] and params[\"use_stemming\"]: \n",
    "        filename = os.path.join(output_folder, \"Resume_proc_lemm_stem.csv\")\n",
    "    else:\n",
    "        filename = os.path.join(output_folder, \"Resume_proc_stem.csv\")\n",
    "    \n",
    "    # Save the DataFrame to the specified folder\n",
    "    df_copy.to_csv(filename, index=False)\n",
    "\n",
    "    print(f\"Processed DataFrame saved as: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different type of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Resume_proc_lemm.csv\n",
      "33524\n",
      "['avert' 'averted' 'averting' 'avery' 'aveta' 'avetis' 'avett' 'avg'\n",
      " 'avian' 'aviara' 'aviatiaon' 'aviation' 'aviationaviation' 'aviator'\n",
      " 'avid' 'avide' 'avila' 'avimark' 'avinashilingam' 'avionic' 'avionics'\n",
      " 'avis' 'avisena' 'aviso' 'aviva' 'avl' 'avma' 'avnet' 'avo' 'avogadro'\n",
      " 'avoid' 'avoidable' 'avoidance' 'avoided' 'avoiding' 'avon' 'avondale'\n",
      " 'avp' 'avt' 'avum' 'awaiting' 'award' 'awarded' 'awardee' 'awardees'\n",
      " 'awarding' 'awardminor' 'aware' 'awareness' 'away' 'awd' 'awe' 'aweber'\n",
      " 'awesome' 'awf' 'awk' 'awkward' 'awol' 'awps' 'awr' 'aws' 'awse' 'awt'\n",
      " 'awv' 'ax' 'axc' 'axial' 'axiom' 'axios' 'axis' 'axle' 'axsun' 'axtel'\n",
      " 'axure' 'ayala' 'ayanniyi' 'ayoola' 'ayoub' 'ayp' 'ayrep' 'ayres'\n",
      " 'ayroll' 'ayso' 'ayuda' 'az' 'azccr' 'azhar' 'azimuth' 'aziz' 'aznn'\n",
      " 'azrouel' 'azteca' 'azure' 'azz' 'azzure' 'años' 'ba' 'baa' 'baan' 'bab']\n",
      "\n",
      "\n",
      "\n",
      "Resume_proc_lemm_stem.csv\n",
      "33524\n",
      "['avert' 'averted' 'averting' 'avery' 'aveta' 'avetis' 'avett' 'avg'\n",
      " 'avian' 'aviara' 'aviatiaon' 'aviation' 'aviationaviation' 'aviator'\n",
      " 'avid' 'avide' 'avila' 'avimark' 'avinashilingam' 'avionic' 'avionics'\n",
      " 'avis' 'avisena' 'aviso' 'aviva' 'avl' 'avma' 'avnet' 'avo' 'avogadro'\n",
      " 'avoid' 'avoidable' 'avoidance' 'avoided' 'avoiding' 'avon' 'avondale'\n",
      " 'avp' 'avt' 'avum' 'awaiting' 'award' 'awarded' 'awardee' 'awardees'\n",
      " 'awarding' 'awardminor' 'aware' 'awareness' 'away' 'awd' 'awe' 'aweber'\n",
      " 'awesome' 'awf' 'awk' 'awkward' 'awol' 'awps' 'awr' 'aws' 'awse' 'awt'\n",
      " 'awv' 'ax' 'axc' 'axial' 'axiom' 'axios' 'axis' 'axle' 'axsun' 'axtel'\n",
      " 'axure' 'ayala' 'ayanniyi' 'ayoola' 'ayoub' 'ayp' 'ayrep' 'ayres'\n",
      " 'ayroll' 'ayso' 'ayuda' 'az' 'azccr' 'azhar' 'azimuth' 'aziz' 'aznn'\n",
      " 'azrouel' 'azteca' 'azure' 'azz' 'azzure' 'años' 'ba' 'baa' 'baan' 'bab']\n",
      "\n",
      "\n",
      "\n",
      "Resume_proc_stem.csv\n",
      "26770\n",
      "['bestsel' 'bet' 'beta' 'beth' 'bethani' 'bethel' 'bethesda' 'bethun'\n",
      " 'betsey' 'better' 'betteru' 'betti' 'bevel' 'beverag' 'beveragesclean'\n",
      " 'beverli' 'bex' 'bexar' 'bextra' 'beyonc' 'beyond' 'bez' 'bezel' 'bfa'\n",
      " 'bfsi' 'bge' 'bgp' 'bh' 'bha' 'bhadruka' 'bhagawan' 'bham' 'bharath'\n",
      " 'bharathanatyam' 'bharati' 'bharatiya' 'bhcg' 'bhen' 'bhopal' 'bhsf' 'bi'\n",
      " 'bia' 'biannual' 'bib' 'bibiana' 'bibl' 'biblic' 'bibliographi' 'bic'\n",
      " 'bicarbon' 'bicycl' 'bid' 'bidaili' 'bidang' 'bidco' 'biddabl' 'bidder'\n",
      " 'bide' 'bidirect' 'bidtellect' 'bidwel' 'biennial' 'biever' 'big'\n",
      " 'bigfix' 'bigfoot' 'bigg' 'bigger' 'biggest' 'bighorn' 'biginsight'\n",
      " 'bigip' 'bigmac' 'bigmachin' 'bigsql' 'bijapur' 'bike' 'biker' 'bilal'\n",
      " 'bilater' 'bilingu' 'bilirubin' 'biliter' 'biliti' 'bill' 'billabl'\n",
      " 'billabong' 'billboard' 'biller' 'billet' 'billiard' 'billig' 'billion'\n",
      " 'billon' 'biloxi' 'biltmor' 'bim' 'bimonthli' 'bin' 'binari']\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all CSV files in the folder\n",
    "input_folder = \"processed_data\"\n",
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Loop through the files and create a DataFrame for each one\n",
    "for file in csv_files:\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    print(file)\n",
    "\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    \n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    count_vect = CountVectorizer(\n",
    "        lowercase=True,\n",
    "        binary = False,\n",
    "        # stop_words = list(stop_words),      # TODO could be not useful, we already remove stop words in preprocess_text\n",
    "        # ngram_range=(1,2),               # Considers unigrams and bigrams\n",
    "        # max_df = 0.85,                    # Ignores words appearing in more than 85% of documents (too common)\n",
    "        # min_df = 2,                       # Keeps words appearing in at least 2 documents (filters rare words)\n",
    "    )\n",
    "    \n",
    "    countvectorizer_train = count_vect.fit_transform(df['Resume_str']).astype(float)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    print(len(count_vect.vocabulary_))\n",
    "\n",
    "    tokens = count_vect.get_feature_names_out()\n",
    "    print(tokens[:100])\n",
    "    print(tokens[-100:])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
